{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Dimensional Optimization\n",
    "## February 3rd, 2022\n",
    "### Overview: Using 1D Optimization methods to optimize 1D functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autograd import numpy as anp\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "def golden_section(f, a, b, tol=1e-5, maxiter=15):\n",
    "    \"\"\"Use the golden section search to minimize the unimodal function f.\n",
    "\n",
    "    Parameters:\n",
    "        f (function): A unimodal, scalar-valued function on [a,b].\n",
    "        a (float): Left bound of the domain.\n",
    "        b (float): Right bound of the domain.\n",
    "        tol (float): The stopping tolerance.\n",
    "        maxiter (int): The maximum number of iterations to compute.\n",
    "\n",
    "    Returns:\n",
    "        (float): The approximate minimizer of f.\n",
    "        (bool): Whether or not the algorithm converged.\n",
    "        (int): The number of iterations computed.\n",
    "    \"\"\"\n",
    "    #initializing converged bool to false\n",
    "    con = False\n",
    "    \n",
    "    #alg 4.1\n",
    "    #initializing variables\n",
    "    x0 = (a+b)/2\n",
    "    p = (1+np.sqrt(5))/2\n",
    "    \n",
    "    for i in range(1,maxiter+1):\n",
    "        #finding c, a tilde, and b tilde\n",
    "        c = (b-a)/p\n",
    "        aT = b - c\n",
    "        bT = a + c\n",
    "        \n",
    "        #changing a or b based on which function value is greater\n",
    "        if f(aT) <= f(bT):\n",
    "            b = bT\n",
    "        else:\n",
    "            a = aT\n",
    "        x1 = (a+b)/2\n",
    "        \n",
    "        #checking tol\n",
    "        if abs(x0 - x1) < tol:\n",
    "            con = True\n",
    "            break\n",
    "            \n",
    "        #resetting variable\n",
    "        x0 = x1\n",
    "            \n",
    "    return x1, con, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : np.exp(x) - 4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.385998267147321, False, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_section(f,0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "def newton1d(df, d2f, x0, tol=1e-5, maxiter=15):\n",
    "    \"\"\"Use Newton's method to minimize a function f:R->R.\n",
    "\n",
    "    Parameters:\n",
    "        df (function): The first derivative of f.\n",
    "        d2f (function): The second derivative of f.\n",
    "        x0 (float): An initial guess for the minimizer of f.\n",
    "        tol (float): The stopping tolerance.\n",
    "        maxiter (int): The maximum number of iterations to compute.\n",
    "\n",
    "    Returns:\n",
    "        (float): The approximate minimizer of f.\n",
    "        (bool): Whether or not the algorithm converged.\n",
    "        (int): The number of iterations computed.\n",
    "    \"\"\"\n",
    "    #initializing converged bool to false\n",
    "    con = False\n",
    "    \n",
    "    #running 1d newton method\n",
    "    for k in range(maxiter+1):\n",
    "        x1 = x0 - df(x0)/d2f(x0)\n",
    "        \n",
    "        #checking tol\n",
    "        if abs(x1 - x0) < tol:\n",
    "            con = True\n",
    "            break\n",
    "            \n",
    "        #resetting variable\n",
    "        x0 = x1\n",
    "        \n",
    "    return x1, con, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lambda x : 2*x + 5*np.cos(5*x)\n",
    "d2f = lambda x : 2 - 25*np.sin(5*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.4473142236328096, True, 47)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton1d(df,d2f,0,maxiter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3\n",
    "def secant1d(df, x0, x1, tol=1e-5, maxiter=15):\n",
    "    \"\"\"Use the secant method to minimize a function f:R->R.\n",
    "\n",
    "    Parameters:\n",
    "        df (function): The first derivative of f.\n",
    "        x0 (float): An initial guess for the minimizer of f.\n",
    "        x1 (float): Another guess for the minimizer of f.\n",
    "        tol (float): The stopping tolerance.\n",
    "        maxiter (int): The maximum number of iterations to compute.\n",
    "\n",
    "    Returns:\n",
    "        (float): The approximate minimizer of f.\n",
    "        (bool): Whether or not the algorithm converged.\n",
    "        (int): The number of iterations computed.\n",
    "    \"\"\"\n",
    "    #initializing converged bool to false\n",
    "    con = False\n",
    "    \n",
    "    #running secant algorithm\n",
    "    for k in range(1,maxiter+1):\n",
    "        DF0 = df(x0)\n",
    "        DF1 = df(x1)\n",
    "        \n",
    "        x2 = (x0*DF1 - x1*DF0) / (DF1 - DF0)\n",
    "        \n",
    "        #checking tol\n",
    "        if abs(x2-x1) < tol:\n",
    "            con = True\n",
    "            break\n",
    "        \n",
    "        #resetting variables\n",
    "        x0 = x1\n",
    "        x1 = x2\n",
    "    \n",
    "    return x2, con, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lambda x: 2*x + np.cos(x) + 10*np.cos(10*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.16367721846481662, True, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secant1d(df,0.,-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4\n",
    "def backtracking(f, Df, x, p, alpha=1, rho=.9, c=1e-4):\n",
    "    \"\"\"Implement the backtracking line search to find a step size that\n",
    "    satisfies the Armijo condition.\n",
    "\n",
    "    Parameters:\n",
    "        f (function): A function f:R^n->R.\n",
    "        Df (function): The first derivative (gradient) of f.\n",
    "        x (float): The current approximation to the minimizer.\n",
    "        p (float): The current search direction.\n",
    "        alpha (float): A large initial step length.\n",
    "        rho (float): Parameter in (0, 1).\n",
    "        c (float): Parameter in (0, 1).\n",
    "\n",
    "    Returns:\n",
    "        alpha (float): Optimal step size.\n",
    "    \"\"\"\n",
    "    #running alg 4.2\n",
    "    Dfp = np.dot(Df((x).T), p)\n",
    "    fx = f(x)\n",
    "    \n",
    "    #keep getting new alpha\n",
    "    while f(x + (alpha*p)) > (fx + c*alpha*Dfp):\n",
    "        alpha = rho * alpha\n",
    "        \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04710128697246249\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x[0]**2 + x[1]**2 + x[2]**2\n",
    "Df = lambda x: np.array([2*x[0], 2*x[1], 2*x[2]])\n",
    "x = np.array([150., .03, 40.])\n",
    "p = np.array([-.5, -100., -4.5])\n",
    "print(backtracking(f, Df, x, p, alpha=1, rho=.9, c=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04710128697246249"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtracking(f,Df,x,p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
